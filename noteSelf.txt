To send data from client to server component one of the ways is to use .bind method
https://nextjs.org/docs/13/app/api-reference/functions/server-actions

WHen using spaces in searchParams, the spaces are encoded with % and numbers like 20, so decode them we use decodeURIComponent a method which will decode the search Params

Delay or latency while performing CRUD operations in the app was most likely at vercel end not your app, as when deployed in render it was running smoothly as expected

One of the problem - when i delete or add bookmark, later when i tried to edit any one of other bookmark, lets say the bookmark next to the deleted bookmark, when the update box opens, the contents of the deleted bookmark shows up, the selected bookmark contents should have been opened, 
- I thought the reason could be that when server parent component rerenders the client child component wasn't rendering, so i did console log and turns out its true, 
- but later when i put proper keys as bookmark id for each of the child component, it worked properly, even though the child component wasn't rerendered when parent component was rerendered, Ig the html doc is created on server itself, but only the client iteractivity part is executed in client side
- So when i searched, the key component actually helped in cases where i m using components passing props, to make sure the child components also rerenders in case there's any change

Problem while setting up the google auth feature
- All i had to do was add the application code , but needed to to be added in client component only 
- and add the code in auth/callback route.turns- Also add the domains which i was accessing in the supabase 'redirect url' dashboard


Shadn ui components
- https://blocks.starterkitpro.com/?ref=dailydev

Optimized bookmarking feature, reducing page load time from 5 seconds to 2-3 seconds by fetching metadata during the add operation instead of the get operation.

- One of the code practice always refactor and never ever repeat code like rewrite the same code, make use of fn then 

- While creating extension, i thought of extracting metadata from the extension, i.e client side only, but extracting metadata requires packages which can run only in server side 

Resource about forming backend api routes nextjs
- https://nextjs.org/docs/app/api-reference/file-conventions/route

While building extension, one of the major problems faced was the CORS error and fighuring out how to create backend as we cant create backend in extension
Some of the ways were creating serverless functions, or creating backend on main machine(which i did at first), it works better in local, as ppl wont be able to access it apart from local 
- To fix the cors issue i had to write the headers in next.config.mjs, and due to ONE MISSING COMMA, wasted 6 hours

WHen i considered of creating serverless function for my extension backend, there were various options to choose, appscript - which wasn't suitable for my backend tasks, supabase, vercel - they had lots of latency(source reddit posts), 
- AT last i chose cloudflare which seem to have less latency, as my priority was less latency and good performance.
- When i started buiding backend, the technologies i thought of using was nodejs as i just wanted to make one API call, and so some authentication, while adding to the DB due to the RLS added to my supabase db.
- WHen I created nextjs cloudflare worker, at first i referred to docs, there were various options to setup cloudflare worker, cli or dashboard and there were other options which i didn't bother to look
- I started with terminal, followed the commands as they were given, it was just a hello world js script. When i tried to import node package, it showed me error, so esm syntax error, at first i thought it was related to import syntax issue, or node version, after researching deeply, i learnt that cloudflare worker supports only edge runtime, on nextjs, meaning nextjs has 2 types of runtime, edge runtime and nodejs runtime.
- SO i created another project using nextjs and deployed on cloudflare worker, i copy pasted the code i did on my main app, as earlier i tried to integrate my main app with extension. AFter copy paste and deploy, when i tried to add bookmark to my deployed backend, again CORS error, even though it was working fine on local. 
- So later after researching on the error as to why its happening on cloudfare only, i read somewhere to use the official template of nextjs to deploy on cloudflare, which was OPENNEXT, 
- Again i created nextjs app this time from OPENNEXT, the steps were lil bit extra, as i had to write more commands (https://opennext.js.org/cloudflare/get-started)
- after writing code and while i tried to deploy, again ERROR, but this time during deploying code, it was giving warning as OPENENEXT doesn't work smoothly or properly on Windows, so they suggested to use other OS, i switched to wsl
- Now i had to look steps to install ubuntu WSL on windows and, after downloading wsl and looked for the dir where i can access my backend repo in the ubuntu terminal, tried to deploy using "npm run deploy"
- And it worked smoothly, again when i tried to add bookmark from my extension, an error, but this time it was different error, as i checked logs on cloudflare platform, and i didn't understand anyting, asked gpt what the error is, learnt after so many failures, that, when we use external packages(like lodash), what npm does is it use api of the package to run it, and in cloudflare it doesn't use nodejs environment it uses some custom js environment, so maybe thats why not all  packages are supported on cloudflare workers. 
- And in my backend project, the url metadata package wasn't working in cloudlfare, and nextResponse too and supabase createClient ones, so i had to remove the imports of those package and make use of dynamic imports to access them.
- after many hours of debugging, when i deployed again, the cors error was gone, but another error was shown, which was rls violated.
- This was because i didn't make use of supabase/ssr package instead used dynamic import from supabase@js related package, also i had to include the bearer token in the createClient method to authenticate
- After making the changes, it finally worked, extension was finally able to make api calls to deployed backend

Now to create the semantic search engine, i learnt the functioning of search engine
Then after digging more, i learnt i just need to add semantic search feature instead of creating the engine
After watching several videos, i learnt that the to create semantic search feature, i need a vector database, there are several options, like pinecone(fast), elastic search(scalable), and i found pg vector, in supabase 

And to get the data, to scrape it, there were several options, selenium and also pupetter, also playwright(made by the same devs who created pupetter)

I didn't make use of selenium or pupetter as they would make my app more complex, so i just get the metadata that is the title of the sites, and then used google gemini stable model for creating the vector of the data, to store in the postgres vector database, in supabase
- The problem i faced was not creating proper error handling, and not creating the vector of correct dimension, to solve this, i had to define a dimension in the table, and then create the appropriate dimension by the gemini model, and then push the data in database. 
- The root cause was, instead of passing an array of integers representing dimensions for vector, i was passing array of objects.

Then i created the UI for the search, and also created api route api/search to get the search results, one of the problems i faced was while sending response back to the client component, i wasn't able to read the response as it showed as a promise, to solve i had to make the response using NextResponse.json() 
Later one of the problems i faced was that since was doing semantic search, the results were calculated from all the bookmarks in database, but i wanted the results only from bookmarks of that user using, for that purpose i added user id as parameter in the supabase function, so it ensures to find similar bookmark of that user only 